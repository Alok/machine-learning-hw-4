\documentclass[11pt, parindent=0]{article}
\usepackage{mathrsfs,listings,hyperref,backref,amsmath,amsfonts,textcomp,amssymb,geometry,graphicx,enumerate,algorithm,algorithmicx,pdfsync}
\usepackage[noend]{algpseudocode}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[parfill]{parskip}
\graphicspath{{./figs/}}
\pagenumbering{roman}
\synctex=1

\def\Name{Alok Singh}
\def\SID{24456212}
\def\Homework{4}
\def\Session{Spring 2016}
\def\Class{CS 189}

\title{\Class --- \Session --- HW \Homework\ Solutions} \author{\Name, SID
\SID}
\date{}
\textheight=9in
\textwidth=6.5in
\topmargin=-.75in
\leftmargin=0.1in
\rightmargin=0.1in
\oddsidemargin=0.25in
\evensidemargin=0.25in

% Custom commands go here
\let\bf\textbf
\let\a\alpha
\let\t\cdot
\newcommand{\one}{\textbf{1}}
\newcommand{\x}{\textbf{X}}
\newcommand{\y}{\textbf{y}}
\newcommand{\w}{\textbf{w}}
\newcommand{\ep}{\varepsilon}
\newcommand{\f}{\forall}

\begin{document}
\maketitle

\section{1}
\label{sec:1}
\begin{enumerate}[(a)]

    \item We take the gradient w.r.t. both variables. Starting with $\alpha$, we have the following chain of derivations:

        $

        \nabla_{\alpha} = \one^T(\x\w+\a \one - \y) + (\w^T\x^T + \a \one^T - \y^T)\one - \one^T (\x \w + \a \one - \y)
        \implies -\one^T\x\w - \a \t n + \one^T\y = \w^T\x^T\one + \a\t n - \y^T\one$

        But $\one^T\y $ is just $\bar{\y}$ and $\w^T\x^T\one$ is 0 as $\bar{\x}=0$. Setting the gradient to 0 gives that $\hat{\a} = \bar{\y}$.

        The simplified gradient w.r.t. $\w$ (set to 0) gives $ 0 = \bar{\y}\one^T\x - \x^T\y + \x^T\x\w - \lambda \w$. noting that the first term can be rewritten as a product involving $\bar{\textbf{x}}$, we can ignore it as $\x$ is centered. Basic rearrangement and inversion gives the desired statement.


\end{enumerate}

\section{2}
\label{sec:2}

\begin{enumerate}[(a)]
    \item $R(\w^{(0)}) = 1.988$
    \item $ \mu^{(0)} = (.95, .73, .73, .26 )    $
    \item $ \w^{(1)} = (-2, .94, -.68)    $
    \item $ R(\w^{(1)}) = 1.7206    $
    \item $  \mu^{(1)} = (.89, .54, .56, .15)       $
    \item $ \w^{(2)} = (-1.69, 1.91, -.83)    $
    \item $  R(\w^{(2)}) = 1.8546   $
\end{enumerate}

\section{3}
\label{sec:3}


\section{4}
\label{sec:4}

\begin{enumerate}[(a)]
    \item If we note that $\tanh = \frac{\sinh}{\cosh}$, then using the definitions of $\sinh$ and $\cosh$ gives us the desired statement, as the factor of $\frac{1}{2}$ in $\sinh$ and $\cosh$ disappears.

    \item $g'$ is just $\frac{\tanh'}{2}$ as the factor of a half vanishes after differentiating. Using the analogue of the $\tan^2+\sec^2=1$ for hyperbolic functions, we get that the derivative is $ \frac{1-\tanh^2}{2} $.

    \item We find the gradient of $J(\textbf{w})$ in terms of $\textbf{w}$. It turns out to be (with no attempt at simplification):

        $ \sum\limits_{i=1}^{n}
        (y_i \cdot
        \frac
        {1}
        {g(
            \textbf{X}_i \cdot \textbf{w}
    )}
    \cdot \textbf{X}_i)
        -
        ((1-y_i) \cdot \frac{1}{1-g(\textbf{X}_i \cdot \textbf{w})} \cdot g'(\textbf{X}_i \cdot \textbf{w}) \cdot \textbf{X}_i )
        $

        The update equation becomes
        $ w \leftarrow \textbf{w} - \epsilon\nabla_wJ = \textbf{w} -\sum\limits_{i=1}^{n}
        (y_i \cdot
        \frac
        {1}
        {g(
            % \textbf{X_i \cdot w}
        \textbf{X}_i \cdot \textbf{w}
        )}
    \cdot \textbf{X}_i)
    -
    ((1-y_i) \cdot \frac{1}{1-g(\textbf{X}_i \cdot \textbf{w})} \cdot g'(\textbf{X}_i \cdot \textbf{w}) \cdot \textbf{X}_i )
        $
\end{enumerate}

\section{5}
\label{sec:5}

Here are some potential reasons why the linear SVM is not utilizing the new feature well:
\begin{enumerate}[1.]
    \item The data might be high dimensional enough that a quadratic kernel is not enough to sufficiently lift the data.

    \item The STM might also be overfitting on the new features and therefore will do poorly on the entire set of data, but might do very well on just the data near midnight.

\end{enumerate}


Some ways to improve the performance of the SVM are:
\begin{enumerate}[1.]
    \item Play around with its margins by changing the `C' parameter (assuming we're using a soft margin).

    \item Finding the most important components of the data by projecting into a lower dimensional space via the singular value decomposition and using just those to lower the dimensionality.

    \item Just playing around with adding or removing features and cross validating.
\end{enumerate}


\end{document}
